/global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori:/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori:/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori:/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori:/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori:/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori:/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori:/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori:/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/bin/python
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
INFO:tensorflow:Logical CPU cores: 28
INFO:tensorflow:Logical CPU cores: 28
I0614 15:47:46.856859 46912496713088 resnet_run_loop.py:212] Logical CPU cores: 28
INFO:tensorflow:Logical CPU cores: 28
I0614 15:47:46.856841 46912496713088 resnet_run_loop.py:212] Logical CPU cores: 28
INFO:tensorflow:Logical CPU cores: 28
I0614 15:47:46.856859 46912496713088 resnet_run_loop.py:212] Logical CPU cores: 28
INFO:tensorflow:Logical CPU cores: 28
I0614 15:47:46.856846 46912496713088 resnet_run_loop.py:212] Logical CPU cores: 28
INFO:tensorflow:Logical CPU cores: 28
I0614 15:47:46.856860 46912496713088 resnet_run_loop.py:212] Logical CPU cores: 28
INFO:tensorflow:Logical CPU cores: 28
I0614 15:47:46.856914 46912496713088 resnet_run_loop.py:212] Logical CPU cores: 28
I0614 15:47:46.856932 46912496713088 resnet_run_loop.py:212] Logical CPU cores: 28
INFO:tensorflow:TF_GPU_THREAD_COUNT: 1
I0614 15:47:46.857086 46912496713088 resnet_run_loop.py:220] TF_GPU_THREAD_COUNT: 1
INFO:tensorflow:TF_GPU_THREAD_MODE: gpu_private
INFO:tensorflow:TF_GPU_THREAD_COUNT: 1
I0614 15:47:46.857088 46912496713088 resnet_run_loop.py:220] TF_GPU_THREAD_COUNT: 1
INFO:tensorflow:TF_GPU_THREAD_MODE: gpu_private
I0614 15:47:46.857174 46912496713088 resnet_run_loop.py:222] TF_GPU_THREAD_MODE: gpu_private
INFO:tensorflow:TF_GPU_THREAD_COUNT: 1
I0614 15:47:46.857073 46912496713088 resnet_run_loop.py:220] TF_GPU_THREAD_COUNT: 1
INFO:tensorflow:TF_GPU_THREAD_MODE: gpu_private
I0614 15:47:46.857157 46912496713088 resnet_run_loop.py:222] TF_GPU_THREAD_MODE: gpu_private
INFO:tensorflow:TF_GPU_THREAD_COUNT: 1
I0614 15:47:46.857088 46912496713088 resnet_run_loop.py:220] TF_GPU_THREAD_COUNT: 1
INFO:tensorflow:TF_GPU_THREAD_MODE: gpu_private
I0614 15:47:46.857174 46912496713088 resnet_run_loop.py:222] TF_GPU_THREAD_MODE: gpu_private
INFO:tensorflow:TF_GPU_THREAD_COUNT: 1
I0614 15:47:46.857076 46912496713088 resnet_run_loop.py:220] TF_GPU_THREAD_COUNT: 1
INFO:tensorflow:TF_GPU_THREAD_MODE: gpu_private
I0614 15:47:46.857162 46912496713088 resnet_run_loop.py:222] TF_GPU_THREAD_MODE: gpu_private
INFO:tensorflow:Logical CPU cores: 28
I0614 15:47:46.857059 46912496713088 resnet_run_loop.py:212] Logical CPU cores: 28
INFO:tensorflow:TF_GPU_THREAD_COUNT: 1
I0614 15:47:46.857161 46912496713088 resnet_run_loop.py:220] TF_GPU_THREAD_COUNT: 1
I0614 15:47:46.857171 46912496713088 resnet_run_loop.py:222] TF_GPU_THREAD_MODE: gpu_private
INFO:tensorflow:TF_GPU_THREAD_COUNT: 1
I0614 15:47:46.857144 46912496713088 resnet_run_loop.py:220] TF_GPU_THREAD_COUNT: 1
INFO:tensorflow:TF_GPU_THREAD_MODE: gpu_private
I0614 15:47:46.857230 46912496713088 resnet_run_loop.py:222] TF_GPU_THREAD_MODE: gpu_private
INFO:tensorflow:TF_GPU_THREAD_COUNT: 1
INFO:tensorflow:TF_GPU_THREAD_MODE: gpu_private
I0614 15:47:46.857286 46912496713088 resnet_run_loop.py:222] TF_GPU_THREAD_MODE: gpu_private
I0614 15:47:46.857282 46912496713088 resnet_run_loop.py:220] TF_GPU_THREAD_COUNT: 1
INFO:tensorflow:TF_GPU_THREAD_MODE: gpu_private
I0614 15:47:46.857366 46912496713088 resnet_run_loop.py:222] TF_GPU_THREAD_MODE: gpu_private
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

W0614 15:47:46.876542 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

W0614 15:47:46.876539 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

W0614 15:47:46.876538 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0614 15:47:46.876665 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

W0614 15:47:46.876533 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0614 15:47:46.876655 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

W0614 15:47:46.876547 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0614 15:47:46.876675 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

W0614 15:47:46.876552 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0614 15:47:46.876675 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

W0614 15:47:46.876567 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0614 15:47:46.876693 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

W0614 15:47:46.876545 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0614 15:47:46.876668 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0614 15:47:46.876667 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0614 15:47:46.876664 46912496713088 module_wrapper.py:139] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W0614 15:47:47.468419 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W0614 15:47:47.468519 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W0614 15:47:47.468516 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W0614 15:47:47.468515 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

W0614 15:47:47.468693 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

W0614 15:47:47.468794 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

W0614 15:47:47.468785 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

W0614 15:47:47.468789 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W0614 15:47:47.468799 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

W0614 15:47:47.469131 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W0614 15:47:47.469160 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W0614 15:47:47.469313 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

W0614 15:47:47.469412 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W0614 15:47:47.469389 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:572: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

W0614 15:47:47.469558 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

W0614 15:47:47.469647 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:583: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpyjazsf4k
W0614 15:47:47.469655 46912496713088 estimator.py:1821] Using temporary folder as model directory: /tmp/tmpyjazsf4k
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpvrbci6a1
W0614 15:47:47.469752 46912496713088 estimator.py:1821] Using temporary folder as model directory: /tmp/tmpvrbci6a1
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp0qquij7m
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmprvvihkzg
W0614 15:47:47.469789 46912496713088 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp0qquij7m
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpucp0ell7
W0614 15:47:47.469818 46912496713088 estimator.py:1821] Using temporary folder as model directory: /tmp/tmprvvihkzg
W0614 15:47:47.469852 46912496713088 estimator.py:1821] Using temporary folder as model directory: /tmp/tmpucp0ell7
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp8j40__mm
W0614 15:47:47.469945 46912496713088 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8j40__mm
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpyjumh66l
W0614 15:47:47.470078 46912496713088 estimator.py:1821] Using temporary folder as model directory: /tmp/tmpyjumh66l
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp6ed32zz0
INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp8j40__mm', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 86400, '_session_config': intra_op_parallelism_threads: 7
inter_op_parallelism_threads: 27
gpu_options {
  visible_device_list: "0"
}
allow_soft_placement: true
graph_options {
  optimizer_options {
    global_jit_level: ON_1
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aab025c7410>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0614 15:47:47.470177 46912496713088 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp6ed32zz0
I0614 15:47:47.470227 46912496713088 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8j40__mm', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 86400, '_session_config': intra_op_parallelism_threads: 7
inter_op_parallelism_threads: 27
gpu_options {
  visible_device_list: "0"
}
allow_soft_placement: true
graph_options {
  optimizer_options {
    global_jit_level: ON_1
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aab025c7410>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
WARNING:tensorflow:'cpuinfo' not imported. CPU info will not be logged.
WARNING:tensorflow:'cpuinfo' not imported. CPU info will not be logged.
W0614 15:47:47.470948 46912496713088 logger.py:387] 'cpuinfo' not imported. CPU info will not be logged.
W0614 15:47:47.470948 46912496713088 logger.py:387] 'cpuinfo' not imported. CPU info will not be logged.
WARNING:tensorflow:'cpuinfo' not imported. CPU info will not be logged.
W0614 15:47:47.471041 46912496713088 logger.py:387] 'cpuinfo' not imported. CPU info will not be logged.
WARNING:tensorflow:'cpuinfo' not imported. CPU info will not be logged.
W0614 15:47:47.471139 46912496713088 logger.py:387] 'cpuinfo' not imported. CPU info will not be logged.
WARNING:tensorflow:'cpuinfo' not imported. CPU info will not be logged.
WARNING:tensorflow:'cpuinfo' not imported. CPU info will not be logged.
WARNING:tensorflow:'cpuinfo' not imported. CPU info will not be logged.
W0614 15:47:47.471216 46912496713088 logger.py:387] 'cpuinfo' not imported. CPU info will not be logged.
W0614 15:47:47.471236 46912496713088 logger.py:387] 'cpuinfo' not imported. CPU info will not be logged.
W0614 15:47:47.471227 46912496713088 logger.py:387] 'cpuinfo' not imported. CPU info will not be logged.
WARNING:tensorflow:'cpuinfo' not imported. CPU info will not be logged.
W0614 15:47:47.471332 46912496713088 logger.py:387] 'cpuinfo' not imported. CPU info will not be logged.
INFO:tensorflow:Benchmark run: {'model_name': 'resnet', 'dataset': {'name': 'ImageNet'}, 'machine_config': {'memory_total': 403611475968, 'memory_available': 360322269184}, 'test_id': None, 'run_date': '2021-06-14T22:47:47.470443Z', 'tensorflow_version': {'version': '1.15.0', 'git_hash': 'v1.15.0-rc3-22-g590d6ee'}, 'tensorflow_environment_variables': [{'name': 'TF_GPU_THREAD_COUNT', 'value': '1'}, {'name': 'TF_GPU_THREAD_MODE', 'value': 'gpu_private'}, {'name': 'TF_XLA_FLAGS', 'value': 'tf_xla_cpu_global_jit'}], 'run_parameters': [{'name': 'batch_size', 'long_value': 64}, {'name': 'dtype', 'string_value': "<dtype: 'float16'>"}, {'name': 'num_workers', 'long_value': 8}, {'name': 'resnet_size', 'string_value': '50'}, {'name': 'resnet_version', 'string_value': '1'}, {'name': 'synthetic_data', 'bool_value': 'False'}, {'name': 'train_epochs', 'long_value': 5}]}
I0614 15:47:47.508271 46912496713088 logger.py:152] Benchmark run: {'model_name': 'resnet', 'dataset': {'name': 'ImageNet'}, 'machine_config': {'memory_total': 403611475968, 'memory_available': 360322269184}, 'test_id': None, 'run_date': '2021-06-14T22:47:47.470443Z', 'tensorflow_version': {'version': '1.15.0', 'git_hash': 'v1.15.0-rc3-22-g590d6ee'}, 'tensorflow_environment_variables': [{'name': 'TF_GPU_THREAD_COUNT', 'value': '1'}, {'name': 'TF_GPU_THREAD_MODE', 'value': 'gpu_private'}, {'name': 'TF_XLA_FLAGS', 'value': 'tf_xla_cpu_global_jit'}], 'run_parameters': [{'name': 'batch_size', 'long_value': 64}, {'name': 'dtype', 'string_value': "<dtype: 'float16'>"}, {'name': 'num_workers', 'long_value': 8}, {'name': 'resnet_size', 'string_value': '50'}, {'name': 'resnet_version', 'string_value': '1'}, {'name': 'synthetic_data', 'bool_value': 'False'}, {'name': 'train_epochs', 'long_value': 5}]}
INFO:tensorflow:Starting cycle: 0/5
I0614 15:47:47.508462 46912496713088 resnet_run_loop.py:706] Starting cycle: 0/5
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0614 15:47:47.512044 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0614 15:47:47.512105 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0614 15:47:47.512102 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0614 15:47:47.512098 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0614 15:47:47.512114 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0614 15:47:47.512295 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0614 15:47:47.512446 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0614 15:47:47.512601 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
INFO:tensorflow:datasets_num_private_threads: 25
I0614 15:47:47.545489 46912496713088 resnet_run_loop.py:89] datasets_num_private_threads: 25
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W0614 15:47:50.148968 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W0614 15:47:50.150776 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W0614 15:47:50.150829 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W0614 15:47:50.150929 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W0614 15:47:50.151265 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W0614 15:47:50.151906 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W0614 15:47:50.153235 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W0614 15:47:50.153630 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/imagenet_preprocessing.py:76: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
2021-06-14 15:47:50.167408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-06-14 15:47:50.168712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-06-14 15:47:50.169155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-06-14 15:47:50.169156: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-06-14 15:47:50.169202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-06-14 15:47:50.171371: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-06-14 15:47:50.171537: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-06-14 15:47:50.171781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-06-14 15:47:50.897460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:50.899610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1b:00.0
2021-06-14 15:47:50.901973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3d:00.0
2021-06-14 15:47:50.904290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3e:00.0
2021-06-14 15:47:50.906629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 4 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:88:00.0
2021-06-14 15:47:50.908593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 5 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:89:00.0
2021-06-14 15:47:50.910588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 6 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b2:00.0
2021-06-14 15:47:50.913673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 7 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b3:00.0
2021-06-14 15:47:50.913721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:50.915775: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:50.917537: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:50.918325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:50.919434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:50.921432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1b:00.0
2021-06-14 15:47:50.921982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:50.922521: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:50.924552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:50.924741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3d:00.0
2021-06-14 15:47:50.925363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1b:00.0
2021-06-14 15:47:50.928065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3e:00.0
2021-06-14 15:47:50.928839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3d:00.0
2021-06-14 15:47:50.929268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:50.933235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 4 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:88:00.0
2021-06-14 15:47:50.934338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3e:00.0
2021-06-14 15:47:50.939408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 5 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:89:00.0
2021-06-14 15:47:50.940767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 4 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:88:00.0
2021-06-14 15:47:50.942821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:50.946162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 6 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b2:00.0
2021-06-14 15:47:50.947746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 5 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:89:00.0
2021-06-14 15:47:50.949999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:50.950065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1b:00.0
2021-06-14 15:47:50.953914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 7 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b3:00.0
2021-06-14 15:47:50.953951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:50.955397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 6 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b2:00.0
2021-06-14 15:47:50.955800: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:50.957391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:50.957542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1b:00.0
2021-06-14 15:47:50.957621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3d:00.0
2021-06-14 15:47:50.957804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:50.959999: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:50.961642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 7 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b3:00.0
2021-06-14 15:47:50.961595: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:50.961681: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:50.963521: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:50.964353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3d:00.0
2021-06-14 15:47:50.964536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:50.964596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3e:00.0
2021-06-14 15:47:50.965039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:50.965449: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:50.966181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:50.967555: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:50.969082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:50.971964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3e:00.0
2021-06-14 15:47:50.972115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1b:00.0
2021-06-14 15:47:50.972187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 4 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:88:00.0
2021-06-14 15:47:50.973477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:50.980751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:50.980887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 4 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:88:00.0
2021-06-14 15:47:50.981037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3d:00.0
2021-06-14 15:47:50.981110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 5 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:89:00.0
2021-06-14 15:47:50.990276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1b:00.0
2021-06-14 15:47:50.990347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 5 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:89:00.0
2021-06-14 15:47:50.990577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3e:00.0
2021-06-14 15:47:50.990650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 6 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b2:00.0
2021-06-14 15:47:50.999504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:50.999882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3d:00.0
2021-06-14 15:47:50.999954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 6 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b2:00.0
2021-06-14 15:47:51.000107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 4 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:88:00.0
2021-06-14 15:47:51.000253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 7 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b3:00.0
2021-06-14 15:47:51.000294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:51.002189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:51.003899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:51.004338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:51.006530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:51.008196: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:51.008186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1b:00.0
2021-06-14 15:47:51.008419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3e:00.0
2021-06-14 15:47:51.008492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 7 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b3:00.0
2021-06-14 15:47:51.008536: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:51.008643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 5 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:89:00.0
2021-06-14 15:47:51.010387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:51.011894: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:51.012295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:51.012867: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:51.014406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:51.015797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3d:00.0
2021-06-14 15:47:51.015953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:51.016105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 4 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:88:00.0
2021-06-14 15:47:51.016257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 6 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b2:00.0
2021-06-14 15:47:51.020359: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:51.024623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:3e:00.0
2021-06-14 15:47:51.024949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 5 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:89:00.0
2021-06-14 15:47:51.025103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 7 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b3:00.0
2021-06-14 15:47:51.025146: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:51.026986: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:51.028596: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:51.029023: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:51.031196: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:51.032793: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:51.033196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 4 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:88:00.0
2021-06-14 15:47:51.033508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 6 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b2:00.0
2021-06-14 15:47:51.037361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:51.041949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 5 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:89:00.0
2021-06-14 15:47:51.042268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 7 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b3:00.0
2021-06-14 15:47:51.042321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:51.044088: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:51.045639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:51.046067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:51.048169: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:51.049728: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:51.050395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 6 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b2:00.0
2021-06-14 15:47:51.054098: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:51.057288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-06-14 15:47:51.058969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 7 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:b3:00.0
2021-06-14 15:47:51.059015: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:51.060785: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:51.062306: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:51.062729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:51.064821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:51.066404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:51.070874: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:51.102416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-06-14 15:47:51.109370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-06-14 15:47:51.136733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-06-14 15:47:51.141387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-06-14 15:47:51.148247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-06-14 15:47:51.152641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-06-14 15:47:51.155160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0614 15:47:51.216338 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0614 15:47:51.216839 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
W0614 15:47:51.224495 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
W0614 15:47:51.242192 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0614 15:47:51.260780 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0614 15:47:51.261285 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0614 15:47:51.267625 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0614 15:47:51.268127 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
W0614 15:47:51.268880 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
W0614 15:47:51.275750 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
W0614 15:47:51.286484 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
W0614 15:47:51.293470 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0614 15:47:51.293540 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
INFO:tensorflow:Calling model_fn.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0614 15:47:51.294039 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
I0614 15:47:51.294048 46912496713088 estimator.py:1148] Calling model_fn.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0614 15:47:51.297061 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0614 15:47:51.297531 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
W0614 15:47:51.301761 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0614 15:47:51.304809 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
W0614 15:47:51.305166 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0614 15:47:51.305331 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0614 15:47:51.309863 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0614 15:47:51.310416 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0614 15:47:51.312281 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:97: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0614 15:47:51.312785 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
W0614 15:47:51.313146 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
W0614 15:47:51.318704 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
W0614 15:47:51.319720 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
W0614 15:47:51.320536 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:54: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
W0614 15:47:51.322814 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
W0614 15:47:51.331098 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
W0614 15:47:51.337621 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
W0614 15:47:51.338482 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:519: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
W0614 15:47:52.667898 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0614 15:47:52.688141 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
W0614 15:47:52.701555 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
W0614 15:47:52.718716 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0614 15:47:52.721132 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

W0614 15:47:52.721300 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0614 15:47:52.728243 46912496713088 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0614 15:47:52.738411 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
W0614 15:47:52.740296 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
W0614 15:47:52.740661 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

W0614 15:47:52.753928 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0614 15:47:52.759867 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0614 15:47:52.760540 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0614 15:47:52.760835 46912496713088 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
W0614 15:47:52.770286 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

W0614 15:47:52.771572 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0614 15:47:52.778499 46912496713088 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
W0614 15:47:52.778760 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0614 15:47:52.790184 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

W0614 15:47:52.792720 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

W0614 15:47:52.793674 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0614 15:47:52.798732 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0614 15:47:52.799621 46912496713088 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0614 15:47:52.800702 46912496713088 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

W0614 15:47:52.823642 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0614 15:47:52.830730 46912496713088 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

W0614 15:47:52.832242 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0614 15:47:52.839222 46912496713088 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
W0614 15:47:52.864116 46912496713088 deprecation.py:323] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_model.py:546: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0614 15:47:52.885231 46912496713088 deprecation.py:323] From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

W0614 15:47:52.920428 46912496713088 module_wrapper.py:139] From /global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py:323: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0614 15:47:52.927792 46912496713088 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

INFO:tensorflow:Done calling model_fn.
I0614 15:47:55.497886 46912496713088 estimator.py:1150] Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
I0614 15:47:55.498882 46912496713088 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
2021-06-14 15:47:57.350408: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2021-06-14 15:47:57.354625: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2021-06-14 15:47:57.356182: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2021-06-14 15:47:57.356668: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555597e89d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:57.356682: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-14 15:47:57.359952: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2021-06-14 15:47:57.360390: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555597b18a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:57.360409: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-14 15:47:57.374457: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2021-06-14 15:47:57.381405: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2021-06-14 15:47:57.381799: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555559811090 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:57.381809: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-14 15:47:57.402536: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
INFO:tensorflow:Graph was finalized.
I0614 15:47:57.402728 46912496713088 monitored_session.py:240] Graph was finalized.
2021-06-14 15:47:57.403041: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2021-06-14 15:47:57.408232: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2021-06-14 15:47:57.408239: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2021-06-14 15:47:57.408670: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555597c4750 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:57.408683: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-14 15:47:57.408681: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555559810d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:57.408694: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-14 15:47:57.470959: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2021-06-14 15:47:57.485623: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2021-06-14 15:47:57.486030: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555597ed040 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:57.486042: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-14 15:47:57.492861: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2021-06-14 15:47:57.498521: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2021-06-14 15:47:57.498891: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555559cddb60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:57.498899: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-14 15:47:58.061245: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55555975be40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:58.061265: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2021-06-14 15:47:58.064863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:58.064917: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.064944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:58.064953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:58.064962: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:58.064971: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:58.064979: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:58.064988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:58.071069: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555597a4d30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:58.071087: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2021-06-14 15:47:58.071194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2021-06-14 15:47:58.071246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.073282: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2021-06-14 15:47:58.074253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:58.074311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.074330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:58.074338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:58.074345: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:58.074352: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:58.074359: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:58.074367: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:58.080503: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2021-06-14 15:47:58.081135: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55555dac3a60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:58.081143: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-14 15:47:58.083250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2021-06-14 15:47:58.083303: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.092458: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555597e2c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:58.092477: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2021-06-14 15:47:58.135550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:58.135633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.135665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:58.135674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:58.135683: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:58.135692: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:58.135700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:58.135710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:58.141220: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555597c9940 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:58.141246: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2021-06-14 15:47:58.141352: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555559816450 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:58.141368: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2021-06-14 15:47:58.152350: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555597e91d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:58.152365: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2021-06-14 15:47:58.153398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:58.153436: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.153453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:58.153460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:58.153467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:58.153474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:58.153480: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:58.153488: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:58.153726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:58.153759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.153776: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:58.153783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:58.153790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:58.153797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:58.153805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:58.153812: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:58.155471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2021-06-14 15:47:58.155515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.161853: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555597eed80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:58.161866: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2021-06-14 15:47:58.164531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:58.164568: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.164586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:58.164593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:58.164600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:58.164607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:58.164621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:58.164628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:58.171553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2021-06-14 15:47:58.171584: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.171798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:58.171831: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.171848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:58.171855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:58.171863: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:58.171870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:58.171877: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:58.171884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:58.171915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2021-06-14 15:47:58.171948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.180840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2021-06-14 15:47:58.180870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.188664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2021-06-14 15:47:58.188696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.231719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-14 15:47:58.231744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2021-06-14 15:47:58.231750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2021-06-14 15:47:58.246381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13273 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2021-06-14 15:47:58.260012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-14 15:47:58.260038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2021-06-14 15:47:58.260045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2021-06-14 15:47:58.267098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13261 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2021-06-14 15:47:58.294374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-14 15:47:58.294404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2021-06-14 15:47:58.294410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2021-06-14 15:47:58.302687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13210 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2021-06-14 15:47:58.310329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-14 15:47:58.310355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2021-06-14 15:47:58.310362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2021-06-14 15:47:58.313402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-14 15:47:58.313421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2021-06-14 15:47:58.313427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2021-06-14 15:47:58.313863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13177 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2021-06-14 15:47:58.316490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-14 15:47:58.316513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2021-06-14 15:47:58.316520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2021-06-14 15:47:58.316575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13177 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2021-06-14 15:47:58.317276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-14 15:47:58.317294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2021-06-14 15:47:58.317300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2021-06-14 15:47:58.320473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13177 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2021-06-14 15:47:58.322531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13177 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2021-06-14 15:47:58.352651: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555597b82a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-06-14 15:47:58.352674: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2021-06-14 15:47:58.354410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:1a:00.0
2021-06-14 15:47:58.354451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.354471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-14 15:47:58.354484: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-14 15:47:58.354491: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-14 15:47:58.354499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-14 15:47:58.354506: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-14 15:47:58.354514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-14 15:47:58.356788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2021-06-14 15:47:58.356818: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-14 15:47:58.429396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-14 15:47:58.429429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2021-06-14 15:47:58.429436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2021-06-14 15:47:58.432046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12892 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2021-06-14 15:47:59.782157: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.95G (13905965824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.782969: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.66G (12515368960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.783926: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.49G (11263832064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.784756: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.44G (10137448448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.785518: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.50G (9123703808 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.786294: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.65G (8211333120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.787059: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.88G (7390199808 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.787815: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.19G (6651179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.788566: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.57G (5986061312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.789316: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.02G (5387454976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.790070: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.52G (4848709120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.790826: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.06G (4363837952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.791582: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.66G (3927453952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.792332: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.29G (3534708480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.793086: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.96G (3181237504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.793840: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.67G (2863113728 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.794595: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.40G (2576802304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.795345: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.16G (2319121920 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.796107: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.94G (2087209728 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.796860: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.75G (1878488832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.797612: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.57G (1690639872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.798374: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.42G (1521575936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.799127: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.27G (1369418496 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.799875: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.15G (1232476672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.800625: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.03G (1109229056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.801372: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 952.06M (998306304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.802117: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 856.85M (898475776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.802866: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 771.17M (808628224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.803617: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 694.05M (727765504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.804362: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 624.65M (654989056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.805107: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 562.18M (589490176 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.805852: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 505.96M (530541312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.806609: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 455.37M (477487360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.807362: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 409.83M (429738752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.808119: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 368.85M (386765056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.808873: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 331.96M (348088576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.809629: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 298.77M (313279744 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.847928: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.90G (13852173824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.848979: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.61G (12466956288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.849778: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.45G (11220260864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.850528: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.40G (10098234368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.851263: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.46G (9088410624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.852003: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.62G (8179569152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.852940: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.86G (7361612288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.854119: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.17G (6625451008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.854523: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.87G (13817308672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.855579: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.55G (5962905600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.856192: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.58G (12435577856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.857410: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.00G (5366615040 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.857858: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.42G (11192019968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.858965: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.50G (4829953536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.859334: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.38G (10072817664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.860439: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.05G (4346957824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.860809: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.44G (9065535488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.861920: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.64G (3912261888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.862290: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.60G (8158981632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.863388: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.28G (3521035520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.863754: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.84G (7343083520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.864856: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.95G (3168931840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.865472: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.15G (6608775168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.866706: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.66G (2852038656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.867156: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.54G (5947897344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.868254: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.39G (2566834688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.868618: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.99G (5353107456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.869710: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.15G (2310151168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.870074: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.49G (4817796608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.871178: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.94G (2079136000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.871544: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.04G (4336016896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.872645: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.74G (1871222528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.873008: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.63G (3902415104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.874367: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.57G (1684100352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.874735: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.27G (3512173568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.876043: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.41G (1515690240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.876409: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.94G (3160956160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.877504: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.27G (1364121344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.877868: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.65G (2844860416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.879132: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.14G (1227709184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.879615: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.38G (2560374272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.880809: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.03G (1104938240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.882292: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.15G (2304336896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.883061: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.87G (13817308672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.884391: F tensorflow/stream_executor/cuda/cuda_driver.cc:175] Check failed: err == cudaSuccess || err == cudaErrorInvalidValue Unexpected CUDA error: out of memory
Fatal Python error: Aborted

Thread 0x00002aab2e166700 (most recent call first):
2021-06-14 15:47:59.884462: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 948.38M (994444544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/threading.py", line 296 in wait
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/queue.py", line 170 in get
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/summary/writer/event_file_writer.py", line 159 in run
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/threading.py", line 926 in _bootstrap_inner
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/threading.py", line 890 in _bootstrap

Current thread 0x00002aaaaab3bd80 (most recent call first):
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1443 in _call_tf_sessionrun
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1350 in _run_fn
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1365 in _do_call
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1359 in _do_run
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1180 in _run
2021-06-14 15:47:59.885202: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.93G (2073903104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 956 in run
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/session_manager.py", line 296 in prepare_session
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 647 in create_session
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 878 in create_session
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 1212 in _create_session
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 1207 in __init__
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 725 in __init__
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 1014 in __init__
2021-06-14 15:47:59.885937: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.58G (12435577856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 584 in MonitoredTrainingSession
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1490 in _train_with_estimator_spec
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1195 in _train_model_default
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1161 in _train_model
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 370 in train
  File "/global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py", line 718 in resnet_main
  File "./official/resnet/imagenet_main.py", line 398 in run_imagenet
  File "./official/resnet/imagenet_main.py", line 405 in main
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/absl/app.py", line 250 in _run_main
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/absl/app.py", line 299 in run
  File "./official/resnet/imagenet_main.py", line 411 in <module>
2021-06-14 15:47:59.886682: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 853.54M (895000064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.887475: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.74G (1866512896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.888239: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.42G (11192019968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.888976: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 768.18M (805500160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.889721: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.56G (1679861504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.890489: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.38G (10072817664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.891225: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 691.37M (724950272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.891984: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.41G (1511875328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.892720: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.44G (9065535488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.893458: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 622.23M (652455424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.894206: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.27G (1360687872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.894977: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.60G (8158981632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.895718: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 560.01M (587209984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.896465: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.14G (1224619008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.897205: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.84G (7343083520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.897945: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 504.01M (528488960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.898718: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.03G (1102157056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.899483: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.15G (6608775168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.900227: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 453.61M (475640064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.901024: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 945.99M (991941376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.901793: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.54G (5947897344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.902528: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 408.25M (428076032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.903295: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 851.39M (892747264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.904059: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.99G (5353107456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.905108: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 367.42M (385268480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.906012: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 766.25M (803472640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.906914: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.49G (4817796608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.907840: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 330.68M (346741760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.908975: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 689.63M (723125504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.910158: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.04G (4336016896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.910538: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.87G (13817308672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.911272: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.87G (13817308672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.911606: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 297.61M (312067584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.912799: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 620.66M (650812928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.913951: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.63G (3902415104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.914355: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.58G (12435577856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.915124: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.58G (12435577856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.915508: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 267.85M (280860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.916647: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 558.60M (585731840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.917802: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.27G (3512173568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.918186: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.42G (11192019968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.919044: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.42G (11192019968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.919441: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 241.06M (252774912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.920582: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 502.74M (527158784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.921738: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.94G (3160956160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.922194: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.38G (10072817664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.923054: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.38G (10072817664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.923441: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 216.96M (227497472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.924648: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 452.46M (474443008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.926050: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.65G (2844860416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.926455: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.44G (9065535488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.927235: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.44G (9065535488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.927644: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 195.26M (204747776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.928877: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 407.22M (426998784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.930071: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.38G (2560374272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.930531: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.60G (8158981632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.931423: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.60G (8158981632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.931827: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 175.74M (184273152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.933046: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 366.50M (384299008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.934427: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.15G (2304336896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.934822: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.84G (7343083520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.935696: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.84G (7343083520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.936099: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 158.16M (165846016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.937286: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 329.85M (345869312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.938926: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.93G (2073903104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.939317: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.15G (6608775168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.940192: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.59G (13518464512 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.940532: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.15G (6608775168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.940939: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 142.35M (149261568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.942160: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 296.86M (311282432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.943809: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.74G (1866512896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.944204: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.54G (5947897344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.944966: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.33G (12166618112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.945318: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.54G (5947897344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.945784: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 128.11M (134335488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.946999: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 267.18M (280154368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.948551: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.56G (1679861504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.948944: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.99G (5353107456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.949887: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.20G (10949955584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.950283: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.99G (5353107456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.950684: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 115.30M (120902144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.951918: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 240.46M (252139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.953573: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.41G (1511875328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.954023: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.49G (4817796608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.954808: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.18G (9854959616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.955203: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.49G (4817796608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.955602: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 103.77M (108812032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.956842: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 216.41M (226925312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.958500: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.27G (1360687872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.958895: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.04G (4336016896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.959669: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.26G (8869463040 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.960130: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.04G (4336016896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.960530: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 93.39M (97931008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.961710: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 194.77M (204232960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.963572: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.14G (1224619008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.963966: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.63G (3902415104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.964859: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.43G (7982516736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.965256: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.63G (3902415104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.965655: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 84.05M (88137984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.966906: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 175.29M (183809792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.968514: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.03G (1102157056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.968908: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.27G (3512173568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.969699: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.69G (7184264704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.970096: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.27G (3512173568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.970671: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 75.65M (79324416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.971852: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 157.76M (165428992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.973434: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 945.99M (991941376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.973828: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.94G (3160956160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.974734: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.02G (6465838080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.975130: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.94G (3160956160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.975531: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 68.08M (71392000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.976771: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 141.99M (148886272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.978401: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 851.39M (892747264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.978849: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.65G (2844860416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.979603: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.42G (5819254272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.979983: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.65G (2844860416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.980458: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 61.28M (64252928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.981715: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 127.79M (133997824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.983350: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 766.25M (803472640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.983743: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.38G (2560374272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.984498: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.88G (5237328896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.984961: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.38G (2560374272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.985365: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 55.15M (57827840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.986541: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 115.01M (120598272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.988316: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 689.63M (723125504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.988708: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.15G (2304336896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.989547: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.39G (4713595904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.989944: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.15G (2304336896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.990430: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 49.63M (52045056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.992004: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 103.51M (108538624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.993599: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 620.66M (650812928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.994196: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.93G (2073903104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.994973: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.95G (4242236160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.995376: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.93G (2073903104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.995784: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 44.67M (46840576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.997158: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 93.16M (97684992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:47:59.999494: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 558.60M (585731840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.000085: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.74G (1866512896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.000853: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.56G (3818012416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.001226: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.74G (1866512896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.001645: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 40.20M (42156544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.002568: F tensorflow/stream_executor/cuda/cuda_driver.cc:175] Check failed: err == cudaSuccess || err == cudaErrorInvalidValue Unexpected CUDA error: out of memory
Fatal Python error: Aborted

Thread 0x00002aab2e166700 (most recent call first):
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/threading.py", line 296 in wait
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/queue.py", line 170 in get
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/summary/writer/event_file_writer.py", line 159 in run
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/threading.py", line 926 in _bootstrap_inner
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/threading.py", line 890 in _bootstrap

Current thread 0x00002aaaaab3bd80 (most recent call first):
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1443 in _call_tf_sessionrun
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1350 in _run_fn
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1365 in _do_call
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1359 in _do_run
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1180 in _run
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 956 in run
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/session_manager.py", line 296 in prepare_session
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 647 in create_session
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 878 in create_session
2021-06-14 15:48:00.003731: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 83.84M (87916544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 1212 in _create_session
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 1207 in __init__
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 725 in __init__
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 1014 in __init__
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py", line 584 in MonitoredTrainingSession
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1490 in _train_with_estimator_spec
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1195 in _train_model_default
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1161 in _train_model
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 370 in train
  File "/global/cscratch1/sd/mrowan/ml-performance-benchmark/resnet/cori/official/resnet/resnet_run_loop.py", line 718 in resnet_main
  File "./official/resnet/imagenet_main.py", line 398 in run_imagenet
  File "./official/resnet/imagenet_main.py", line 405 in main
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/absl/app.py", line 250 in _run_main
  File "/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/absl/app.py", line 299 in run
  File "./official/resnet/imagenet_main.py", line 411 in <module>
2021-06-14 15:48:00.005334: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 502.74M (527158784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.005916: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.56G (1679861504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.006760: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.20G (3436211200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.007168: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.56G (1679861504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.007578: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 36.18M (37940992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.008943: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 75.46M (79124992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.010547: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 452.46M (474443008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.011129: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.41G (1511875328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.011912: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.88G (3092590080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.012290: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.41G (1511875328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.012709: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 32.56M (34147072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.014066: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 67.91M (71212544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.015662: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 407.22M (426998784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.016198: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.27G (1360687872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.016956: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.59G (2783331072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.017358: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.27G (1360687872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.017773: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 29.31M (30732544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.019171: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 61.12M (64091392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.020747: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 366.50M (384299008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.021321: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.14G (1224619008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.022081: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.33G (2504997888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.022451: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.14G (1224619008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.022881: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 26.38M (27659520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.024278: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 55.01M (57682432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.025868: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 329.85M (345869312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.026448: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.03G (1102157056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.027234: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.10G (2254498048 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.027638: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.03G (1102157056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.028053: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 23.74M (24893696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.029421: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 49.51M (51914240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.031025: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 296.86M (311282432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.031608: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 945.99M (991941376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.032371: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.89G (2029048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.032774: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 945.99M (991941376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.033190: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 21.37M (22404352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.034573: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 44.56M (46722816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.036178: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 267.18M (280154368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.036730: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 851.39M (892747264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.037494: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.70G (1826143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.037891: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 851.39M (892747264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.038321: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 19.23M (20164096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.039673: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 40.10M (42050560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.041273: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 240.46M (252139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.041821: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 766.25M (803472640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.042674: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.53G (1643529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.043090: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 766.25M (803472640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.043514: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 17.31M (18147840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.044955: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 36.09M (37845504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.046570: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 216.41M (226925312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.047127: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 689.63M (723125504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.047875: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.38G (1479176192 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.048281: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 689.63M (723125504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.048707: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 15.58M (16333056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.050110: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 32.48M (34061056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.051747: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 194.77M (204232960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.052353: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 620.66M (650812928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.053164: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.24G (1331258624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.053567: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 620.66M (650812928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.053994: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 14.02M (14699776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.055492: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 29.23M (30654976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.057101: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 175.29M (183809792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.057719: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 558.60M (585731840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.058501: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.12G (1198132736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.058907: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 558.60M (585731840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.059340: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.62M (13229824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.060840: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 26.31M (27589632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.062487: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 157.76M (165428992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.063103: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 502.74M (527158784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.063880: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.00G (1078319616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.064276: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 502.74M (527158784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.064741: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.36M (11907072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.066158: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 23.68M (24830720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.067797: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 141.99M (148886272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.068411: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 452.46M (474443008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.069202: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 925.53M (970487808 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.069564: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 452.46M (474443008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.070021: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.22M (10716416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.071543: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 21.31M (22347776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.073170: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 127.79M (133997824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.073783: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 407.22M (426998784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.074580: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 832.98M (873438976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.074992: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 407.22M (426998784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.075456: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.20M (9644800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.076720: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 19.18M (20113152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.078439: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 115.01M (120598272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.078890: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 366.50M (384299008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.079718: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 749.68M (786095104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.080092: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 366.50M (384299008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.080630: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.28M (8680448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.081892: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 17.26M (18102016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.083718: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 103.51M (108538624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.084113: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 329.85M (345869312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.084905: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 674.71M (707485696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.085303: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 329.85M (345869312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.085919: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.45M (7812608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.087184: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 15.54M (16291840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.089140: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 93.16M (97684992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.089536: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 296.86M (311282432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.090343: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 607.24M (636737280 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.090720: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 296.86M (311282432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.091338: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.71M (7031552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.092569: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 13.98M (14662656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.094420: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 83.84M (87916544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.094818: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 267.18M (280154368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.095658: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 546.52M (573063680 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.096040: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 267.18M (280154368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.096677: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.04M (6328576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.097897: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.58M (13196544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.099772: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 75.46M (79124992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.100169: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 240.46M (252139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.100975: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 491.86M (515757312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.101338: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 240.46M (252139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.102013: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.43M (5695744 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.103255: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.33M (11877120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.105090: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 67.91M (71212544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.105487: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 216.41M (226925312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.106303: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 442.68M (464181760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.106675: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 216.41M (226925312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.107334: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.89M (5126400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.108634: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.19M (10689536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.110467: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 61.12M (64091392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.110867: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 194.77M (204232960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.111714: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 398.41M (417763584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.112089: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 194.77M (204232960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.112746: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.40M (4613888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.113990: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.17M (9620736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.115885: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 55.01M (57682432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.116284: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 175.29M (183809792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.117158: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 358.57M (375987200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.117521: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 175.29M (183809792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.118251: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.96M (4152576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.119531: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.26M (8658688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.121420: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 49.51M (51914240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.121818: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 157.76M (165428992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.122662: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 322.71M (338388480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.123071: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 157.76M (165428992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.123772: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.56M (3737344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.125035: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.43M (7792896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.127121: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 44.56M (46722816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.127520: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 141.99M (148886272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.128355: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 290.44M (304549632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.128756: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 141.99M (148886272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.129492: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.21M (3363840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.130845: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.69M (7013632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.132957: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 40.10M (42050560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.133356: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 127.79M (133997824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.134191: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 261.40M (274094848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.134610: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 127.79M (133997824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.135313: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.89M (3027456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.136582: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.02M (6312448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.138522: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 36.09M (37845504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.138925: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 115.01M (120598272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.139828: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 235.26M (246685440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.140215: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 115.01M (120598272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.140941: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.60M (2724864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.142256: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.42M (5681408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.144368: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 32.48M (34061056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.144769: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 103.51M (108538624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.145632: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 211.73M (222017024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.146043: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 103.51M (108538624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.146900: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.34M (2452480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.148205: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.88M (5113344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.150317: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 29.23M (30654976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.150720: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 93.16M (97684992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.151614: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 190.56M (199815424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.152016: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 93.16M (97684992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.152839: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.10M (2207232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.154180: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.39M (4602112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.156401: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 26.31M (27589632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.156804: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 83.84M (87916544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.157723: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 171.50M (179834112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.158137: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 83.84M (87916544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.159163: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.89M (1986560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.160530: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.95M (4142080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.162849: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 23.68M (24830720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.163258: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 75.46M (79124992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.164177: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 154.35M (161850880 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.164582: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 75.46M (79124992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.165586: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.71M (1787904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.167000: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.55M (3727872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.169180: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 21.31M (22347776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.169589: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 67.91M (71212544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.170518: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 138.92M (145665792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.170922: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 67.91M (71212544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.171903: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.53M (1609216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.173275: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.20M (3355136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.175515: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 19.18M (20113152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.175927: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 61.12M (64091392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.176872: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 125.03M (131099392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.177279: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 61.12M (64091392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.178340: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.38M (1448448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.179751: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.88M (3019776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.182012: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 17.26M (18102016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.182435: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 55.01M (57682432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.183410: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 112.52M (117989632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.183814: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 55.01M (57682432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.184850: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.24M (1303808 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.186232: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.59M (2717952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.188493: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 15.54M (16291840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.188906: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 49.51M (51914240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.189828: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 101.27M (106190848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.190237: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 49.51M (51914240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.191347: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.12M (1173504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.192800: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.33M (2446336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.195064: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 13.98M (14662656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.195479: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 44.56M (46722816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.196402: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 91.14M (95571968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.196812: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 44.56M (46722816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.197774: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.01M (1056256 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.199185: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.10M (2201856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.201408: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.58M (13196544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.201827: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 40.10M (42050560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.202925: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 82.03M (86014976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.203334: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 40.10M (42050560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.204304: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 928.5K (950784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.205956: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.89M (1981696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.208187: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.33M (11877120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.208606: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 36.09M (37845504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.209751: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 73.83M (77413632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.210171: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 36.09M (37845504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.211155: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 835.8K (855808 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.212791: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.70M (1783552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.215024: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.19M (10689536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.215445: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 32.48M (34061056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.216574: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 66.44M (69672448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.216991: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 32.48M (34061056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.217981: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 752.2K (770304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.219566: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.53M (1605376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.221863: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.17M (9620736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.222286: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 29.23M (30654976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.223378: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 59.80M (62705408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.223793: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 29.23M (30654976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.224864: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 677.2K (693504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.226463: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.38M (1444864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.228700: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.26M (8658688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.229113: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 26.31M (27589632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.230206: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 53.82M (56434944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.230640: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 26.31M (27589632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.231662: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 609.8K (624384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.233257: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.24M (1300480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.235631: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.43M (7792896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.236042: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 23.68M (24830720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.237142: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 48.44M (50791680 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.237560: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 23.68M (24830720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.238586: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 549.0K (562176 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.240186: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.12M (1170432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.242486: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.69M (7013632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.242900: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 21.31M (22347776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.244004: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 43.59M (45712640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.244424: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 21.31M (22347776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.245398: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 494.2K (506112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.247052: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.00M (1053440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.249173: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.02M (6312448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.249588: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 19.18M (20113152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.250790: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 39.24M (41141504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.251210: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 19.18M (20113152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.252041: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 445.0K (455680 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.253934: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 926.0K (948224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.256306: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.42M (5681408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.256737: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 17.26M (18102016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.257984: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 35.31M (37027584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.258432: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 17.26M (18102016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.259243: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 400.5K (410112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.261006: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 833.5K (853504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.263174: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.88M (5113344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.263609: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 15.54M (16291840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.264876: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 31.78M (33325056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.265308: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 15.54M (16291840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.266133: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 360.5K (369152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.267923: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 750.2K (768256 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.270098: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.39M (4602112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.270551: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 13.98M (14662656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.271779: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 28.60M (29992704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.272213: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 13.98M (14662656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.273213: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 324.5K (332288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.275053: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 675.2K (691456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.277265: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.95M (4142080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.277706: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.58M (13196544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.278947: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 25.74M (26993664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.279388: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.58M (13196544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.280221: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 292.2K (299264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.282255: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 607.8K (622336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.284481: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.55M (3727872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.284931: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.33M (11877120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.286186: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 23.17M (24294400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.286657: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.33M (11877120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.287486: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 263.2K (269568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.289322: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 547.0K (560128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.291579: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.20M (3355136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.292029: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.19M (10689536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.293259: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 20.85M (21864960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.293711: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.19M (10689536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.294549: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 237.0K (242688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.296428: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 492.5K (504320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.298679: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.88M (3019776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.299140: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.17M (9620736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.300392: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 18.77M (19678464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.300854: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.17M (9620736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.301683: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 213.5K (218624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.303537: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 443.2K (453888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.305781: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.59M (2717952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.306253: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.26M (8658688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.307501: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 16.89M (17710848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.307960: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.26M (8658688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.308790: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 192.2K (196864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.310781: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 399.0K (408576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.313046: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.33M (2446336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.313524: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.43M (7792896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.314773: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 15.20M (15939840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.315251: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.43M (7792896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.316075: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 173.2K (177408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.317979: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 359.2K (367872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.320274: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.10M (2201856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.320751: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.69M (7013632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.322043: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 13.68M (14345984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.322541: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.69M (7013632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.323351: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 156.0K (159744 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.325416: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 323.5K (331264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.327898: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.89M (1981696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.328376: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.02M (6312448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.329789: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.31M (12911616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.330278: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.02M (6312448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.331112: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 140.5K (143872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.333174: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 291.2K (298240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.335732: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.70M (1783552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.336238: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.42M (5681408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.337491: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.08M (11620608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.337999: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.42M (5681408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.338930: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 126.5K (129536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.340983: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 262.2K (268544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.343498: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.53M (1605376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.344004: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.88M (5113344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.345319: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.97M (10458624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.345828: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.88M (5113344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.346714: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 114.0K (116736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.348791: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 236.2K (241920 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.351293: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.38M (1444864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.351798: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.39M (4602112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.353048: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.98M (9412864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.353556: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.39M (4602112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.354460: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 102.8K (105216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.356568: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 212.8K (217856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.359126: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.24M (1300480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.359689: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.95M (4142080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.360940: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.08M (8471808 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.361504: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.95M (4142080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.362343: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 92.5K (94720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.364628: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 191.5K (196096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.367202: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.12M (1170432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.367765: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.55M (3727872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.369046: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.27M (7624704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.369610: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.55M (3727872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.370457: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 83.2K (85248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.372565: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 172.5K (176640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.375149: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.00M (1053440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.375711: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.20M (3355136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.376993: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.54M (6862336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.377557: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.20M (3355136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.378567: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 75.0K (76800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.380674: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 155.2K (158976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.383280: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 926.0K (948224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.383843: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.88M (3019776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.385137: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.89M (6176256 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.385702: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.88M (3019776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.386592: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 67.5K (69120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.388712: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 139.8K (143104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.391313: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 833.5K (853504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.391876: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.59M (2717952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.393171: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.30M (5558784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.393720: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.59M (2717952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.394560: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 60.8K (62208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.396777: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 126.0K (129024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.399366: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 750.2K (768256 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.399929: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.33M (2446336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.401223: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.77M (5003008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.401766: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.33M (2446336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.402612: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 54.8K (56064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.404737: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 113.5K (116224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.407470: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 675.2K (691456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.408032: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.10M (2201856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.409328: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.29M (4502784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.409871: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.10M (2201856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.410731: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 49.5K (50688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.413010: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 102.2K (104704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.415869: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 607.8K (622336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.416602: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.89M (1981696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.418137: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.86M (4052736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.418882: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.89M (1981696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.419692: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 44.8K (45824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.422071: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 92.2K (94464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.424914: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 547.0K (560128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.425647: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.70M (1783552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.427020: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.48M (3647488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.427750: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.70M (1783552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.428688: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 40.5K (41472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.431000: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 83.2K (85248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.433971: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 492.5K (504320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.434716: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.53M (1605376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.436087: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.13M (3282944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.436798: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.53M (1605376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.437627: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 36.5K (37376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.439988: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 75.0K (76800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.442952: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 443.2K (453888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.443686: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.38M (1444864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.445051: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.82M (2954752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.445798: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.38M (1444864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.446679: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 33.0K (33792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.448959: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 67.5K (69120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.451824: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 399.0K (408576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.452558: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.24M (1300480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.454003: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.54M (2659328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.454748: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.24M (1300480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.455583: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 29.8K (30464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.457904: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 60.8K (62208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.460722: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 359.2K (367872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.461457: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.12M (1170432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.462905: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.28M (2393600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.463618: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.12M (1170432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.464529: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 27.0K (27648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.466795: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 54.8K (56064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.469593: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 323.5K (331264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.470346: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.00M (1053440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.471710: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 2.05M (2154240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.472441: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.00M (1053440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.473330: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 24.5K (25088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.475658: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 49.5K (50688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.478777: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 291.2K (298240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.479511: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 926.0K (948224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.481029: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.85M (1938944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.481764: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 926.0K (948224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.482628: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 22.2K (22784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.484905: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 44.8K (45824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.487927: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 262.2K (268544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.488660: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 833.5K (853504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.490174: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.66M (1745152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.490919: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 833.5K (853504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.491778: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 20.2K (20736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.494054: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 40.5K (41472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.497073: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 236.2K (241920 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.497807: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 750.2K (768256 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.499334: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.50M (1570816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.500066: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 750.2K (768256 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.500924: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 18.2K (18688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.503215: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 36.5K (37376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.506367: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 212.8K (217856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.507101: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 675.2K (691456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.508616: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.35M (1413888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.509330: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 675.2K (691456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.510304: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 16.5K (16896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.512707: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 33.0K (33792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.515665: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 191.5K (196096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.516398: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 607.8K (622336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.517913: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.21M (1272576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.518659: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 607.8K (622336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.519581: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 15.0K (15360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.521871: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 29.8K (30464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.525008: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 172.5K (176640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.525742: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 547.0K (560128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.527396: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1.09M (1145344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.528128: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 547.0K (560128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.529132: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 13.5K (13824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.531421: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 27.0K (27648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.534633: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 155.2K (158976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.535368: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 492.5K (504320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.536977: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 1006.8K (1030912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.537713: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 492.5K (504320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.538723: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.2K (12544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.540982: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 24.5K (25088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.543997: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 139.8K (143104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.544730: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 443.2K (453888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.546271: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 906.2K (928000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.547115: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 443.2K (453888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.547869: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.2K (11520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.550149: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 22.2K (22784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.553157: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 126.0K (129024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.553890: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 399.0K (408576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.555532: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 815.8K (835328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.556355: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 399.0K (408576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.557111: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.2K (10496 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.559401: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 20.2K (20736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.562411: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 113.5K (116224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.563146: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 359.2K (367872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.564712: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 734.2K (751872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.565536: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 359.2K (367872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.566287: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.2K (9472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.568745: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 18.2K (18688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.571690: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 102.2K (104704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.572423: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 323.5K (331264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.573980: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 661.0K (676864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.574829: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 323.5K (331264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.575563: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.5K (8704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.577999: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 16.5K (16896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.580989: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 92.2K (94464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.581721: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 291.2K (298240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.583277: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 595.0K (609280 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.584080: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 291.2K (298240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.584815: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.8K (7936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.587094: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 15.0K (15360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.590069: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 83.2K (85248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.590814: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 262.2K (268544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.592308: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 535.5K (548352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.593092: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 262.2K (268544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.593835: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.0K (7168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.596121: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 13.5K (13824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.599119: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 75.0K (76800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.599853: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 236.2K (241920 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.601534: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 482.0K (493568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.602334: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 236.2K (241920 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.603077: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.5K (6656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.605353: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.2K (12544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.608344: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 67.5K (69120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.609077: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 212.8K (217856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.610705: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 434.0K (444416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.611510: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 212.8K (217856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.612245: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.0K (6144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.614531: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 11.2K (11520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.617529: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 60.8K (62208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.618274: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 191.5K (196096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.619806: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 390.8K (400128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.620633: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 191.5K (196096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.621366: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.5K (5632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.623672: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 10.2K (10496 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.626688: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 54.8K (56064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.627422: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 172.5K (176640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.628937: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 351.8K (360192 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.629769: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 172.5K (176640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.630513: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.0K (5120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.632792: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 9.2K (9472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.635778: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 49.5K (50688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.636512: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 155.2K (158976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.638046: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 316.8K (324352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.638892: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 155.2K (158976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.639626: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.5K (4608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.641899: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 8.5K (8704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.644925: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 44.8K (45824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.645659: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 139.8K (143104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.647195: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 285.2K (292096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.648003: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 139.8K (143104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.648745: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.2K (4352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.651005: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.8K (7936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.654030: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 40.5K (41472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.654775: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 126.0K (129024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.656313: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 256.8K (262912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.657120: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 126.0K (129024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.657853: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.0K (4096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.660142: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 7.0K (7168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.663132: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 36.5K (37376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.663866: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 113.5K (116224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.665401: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 231.2K (236800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.666211: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 113.5K (116224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.666954: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.8K (3840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.669241: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.5K (6656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.672232: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 33.0K (33792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.672965: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 102.2K (104704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.674677: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 208.2K (213248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.675507: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 102.2K (104704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.676241: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.5K (3584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.678537: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 6.0K (6144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.681565: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 29.8K (30464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.682317: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 92.2K (94464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.683857: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 187.5K (192000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.685609: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 92.2K (94464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.686353: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.2K (3328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.688631: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 5.5K (5632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.691697: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 27.0K (27648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.692431: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 83.2K (85248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:00.693967: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 168.8K (172800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:01.317426: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.87G (13817303552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:01.318344: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.87G (13817303552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:01.319239: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.87G (13817283584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:01.320023: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.87G (13817283584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:01.376993: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.87G (13817223424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:01.377757: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.87G (13817223424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:01.387887: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.59G (13518308864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:01.388250: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.87G (13817231872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:01.389330: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.59G (13518308864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-06-14 15:48:01.389694: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 12.87G (13817231872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
